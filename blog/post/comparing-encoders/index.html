<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>Comparing Video Encoders</title>
    <meta
      name="description"
      content="Comparing video encoders is easy, but not often done correctly."
    />
    <meta
      name="keywords"
      content="compression, encoding, AV1, video, codec, VMAF, PSNR, SSIM, SSIMULACRA2, XPSNR"
    />
    <meta property="og:title" content="Comparing Video Encoders" />
    <meta
      property="og:description"
      content="Comparing video encoders is easy, but not often done correctly."
    />
    <meta name="author" content="Gianni Rosato" />
    <meta name="robots" content="index, follow" />
    <meta
      name="googlebot"
      content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"
    />
    <meta
      name="bingbot"
      content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"
    />
    <meta property="og:type" content="article" />
    <meta
      property="og:url"
      content="https://giannirosato.com/blog/post/comparing-encoders/"
    />

    <link href="/static/favicon.webp" rel="icon" />
    <link href="/static/css/main.css" rel="stylesheet" />
    <link href="/static/css/markdown.css" rel="stylesheet" />
    <script
      defer
      src="https://cloud.umami.is/script.js"
      data-website-id="ed9fd2cd-67dd-4524-ad11-1810fb677e29"
    ></script>
  </head>

  <body>
    <div class="center">
      <div class="post-header">
        <div class="title" style="display: inline-grid">
          Comparing Video Encoders
        </div>
        <div class="date">Mar 15, 2025</div>
        <a class="tag" href="/blog/tag/av1/">av1</a>
      </div>
      <div class="button-container-left">
        <a href="/blog/" class="button-blog">Back</a>
        <a href="/" class="button-blog">Home</a>
      </div>

      <div class="summary-box">
        <h3>Summary</h3>
        <p>
          Comparing video encoders involves using synthetic metrics to assess
          visual quality. Tools like "metrics" from the Psychovisual Experts
          Group can be used to help generate data that can be plotted. Plotting
          encoder performance in different ways helps determine the best encoder
          for your implementation needs, considering quality & speed.
        </p>
      </div>

      <h2>Why?</h2>
      <p>
        Comparing video encoders isn't hard; in fact, it is usually quite easy.
        However, it is very often done incorrectly.
      </p>
      <p>
        That's a bit of an oversimplification. Comparing video encoders
        extremely well <em>is</em> rather difficult, and it is the focus of a
        lot of impactful research that aims to produce metrics that can properly
        assess how good a video looks to our eyes. The human eye is very
        complex, and guiding compression algorithms to care about the human
        visual system can get very interesting (I wrote a bit about <a
          href="https://giannirosato.com/blog/post/jpegli-xyb/"
        >perceptual color encoding in JPEG</a> with the XYB colorspace used in
        JPEG XL, it can be very cool stuff).
      </p>
      <p>
        This article is more about what we can do now with the tools that we
        have, regardless of the metric we're interested in. Many people,
        including the SVT-AV1 team, make use of <a
          href="https://wiki.x266.mov/docs/metrics/PSNR"
        >PSNR</a>, <a href="https://wiki.x266.mov/docs/metrics/SSIM">SSIM</a>,
        and <a href="https://wiki.x266.mov/docs/metrics/VMAF">VMAF</a>, but
        today we're going to be (mainly) focusing on <a
          href="https://wiki.x266.mov/docs/metrics/XPSNR"
        >XPSNR</a>, a perceptual metric by Fraunhofer HHI that is readily
        available in FFmpeg 7.1.
      </p>
      <p>Now that we have established the problem space, we can talk about:</p>
      <ul>
        <li>The tools we have available to compute metrics in a useful way</li>
        <li>How we can use them to compare video encoders</li>
      </ul>
      <h2>Tools</h2>
      <p>
        A helpful toolbox of various scripts is provided by the <code
        >metrics</code> utility by the <a href="https://github.com/psy-ex"
        >Psychovisual Experts Group</a>. You can find the code via <a
          href="https://github.com/psy-ex/metrics"
        >this GitHub link</a>.
      </p>
      <p>
        This tool lets us compute some image-focused metrics that we will use
        for video, and Weighted XPSNR, a video metric based on XPSNR that
        includes chroma information (officially, XPSNR is recommended to be
        luma-only).
      </p>
      <h2>Comparisons</h2>
      <p>
        There are three "tiers" of comparisons, each involving a bit more data
        than the last:
      </p>
      <ul>
        <li>Comparing a single video to another single video</li>
        <li>
          Comparing a series of encoders in terms of compression efficiency
        </li>
        <li>Comparing a series of encoders in terms of overall efficiency</li>
      </ul>
      <p>We'll start with simple two-video comparisons.</p>
      <h2>Comparing Two Videos</h2>
      <p>
        XPSNR is what we call a <em>full-reference distortion metric</em>, which
        means we compare a distorted video to its source to get a score. Since
        we're encoding a source video with a video encoder, we can compare the
        source and the encode with <code>scores.py</code>:
      </p>
      <p><code>./scores.py [source] [encode]</code></p>
      <p>
        You can also use <code>encode.py</code> to encode the video for you.
        Either one will give us various statistics for the metrics we have
        available to us. Given we used the GPU for computation of
        SSIMULACRA2/Butteraugli (more on that in a second), you'll get something
        like this output:
      </p>
      <pre
      >
<code>
     SSIMULACRA2 scores for every 1 frame:
     Average:       75.22395
     Harmonic Mean: 75.08624
     Std Deviation: 3.19206
     10th Pctile:   70.52215
     Butteraugli scores for every 1 frame:
     Distance:      0.80522
     Max Distance:  0.97927
     XPSNR scores:
     XPSNR Y:       34.80490
     XPSNR U:       38.48910
     XPSNR V:       37.42110
     W-XPSNR:       35.61793
	</code></pre
      >
      <p>
        You'll notice that this is a lot more than just a single data point.
        We're just supposed to compare two videos and get a number for how the
        encode looks, right? Ideally, yes, but with the imperfect tools we have,
        we must do the best we can.
      </p>
      <a href="https://wiki.x266.mov/docs/metrics/SSIMULACRA2">SSIMULACRA2</a>
      <ul>
        <li>
          The average, or the arithmetic mean, is simple and easy to understand;
          now we know how our frames look, on average, according to an image
          metric.
        </li>
        <li>
          The harmonic mean pulls our average down toward the lower scores
          present in our per-frame score dataset that we're interpreting. This
          is theoretically a bit more informative than the average, as our eyes
          are going to be more sensitive to variability in the video's fidelity,
          so we make consistency desirable by favoring the lower-scoring frames.
          Note that we do some <a
            href="https://github.com/psy-ex/metrics?tab=readme-ov-file#harmonic-mean"
          >funky stuff</a> to allow the harmonic mean to account for negative
          values with our use case.
        </li>
        <li>
          The standard deviation tells us more about the video's consistency.
        </li>
        <li>
          The 10th percentile lets us know how our least desirable frames are
          scoring.
        </li>
      </ul>
      <p>
        So, lots of ways to try to make an image fidelity metric useful for
        video.
      </p>
      <a href="https://github.com/google/butteraugli">Butteraugli</a>
      <p>
        The way we use Butteraugli in <code>metrics</code>, we use 3 p-norm,
        which weighs and averages certain parts of the frame, leaning toward
        more noticeable differences. So for our use case:
      </p>
      <ul>
        <li>The "Distance" is the average of per-frame 3pnorm scores</li>
        <li>
          The "Max Distance" is the maximum 3pnorm score we saw in a frame
        </li>
      </ul>
      <p>And finally, Weighted XPSNR, or W-XPSNR. This is kind of simple:</p>
      <ul>
        <li>Y XPSNR is "real" XPSNR, luma-only</li>
        <li>U & V XPSNR are for chroma</li>
        <li>
          W-XPSNR extrapolates mean square error from each of the three scores
          and computes a weighted average favoring luma, then computes back to
          the dB units that PSNR-derived metrics (and others) use for reporting
          fidelity.
        </li>
      </ul>
      <p>
        So, Weighted XPSNR is just a weighted average for luma and chroma scores
        that aims to fairly favor luma since that is what our eyes care most
        about.
      </p>
      <h2>Comparing Compression Efficiency</h2>
      <p>
        Now we have scores for one video. But, what size is it? How does it
        compare to another video from another encoder that's a slightly
        different size with slightly different scores? You can interpret this
        subjectively, like saying your 1.74MB video at XPSNR 34.03 from Encoder
        A <em>feels</em> like a better option than a 1.81MB video that scores
        34.21 from Encoder B, but how can we know for sure?
      </p>
      <p>
        The best way we can do this is by looking at a curve that plots
        size-to-score for a series of clips, which is meant to allow us to see
        which encoder (or configuration) achieves the best compression
        efficiency.
      </p>
      <p>Here's a plot comparing various SVT-AV1 speed settings:</p>
      <img
        src="/static/images/compression_efficiency.svg"
        alt="SVT-AV1 Speed Plot"
      >
      <p>
        You can see that despite the fact that Preset 4 & Preset 2 produce
        smaller files at each CRF level, they are not the most efficient
        presets, because Preset 0 displays the best compression efficiency
        according to the curve. Each one of these curves came from an invocation
        of <code>stats.py</code> that provided us with the data we wanted.
      </p>
      <p>Here's an example of how to use <code>stats.py</code>:</p>
      <pre
      >
<code>
     ./stats.py \
     -i source.mkv \
     -q "20 21 24 26 30" \
     -o ./stats.csv \
     -g 4 \
     svtav1 -- --preset 8 --tune 2
	</code></pre
      >
      <p>
        This encodes <code>source.mkv</code> at 5 CRF values, then outputs the
        results to <code>stats.csv</code> which include metrics and encode time.
        We use 4 GPU threads, and we pass a couple of options to SVT-AV1.
      </p>
      <p>
        We picked our 5 CRF values by choosing our bounds and the number of
        values we want, according to a formula (in Python):
      </p>
      <p>
        <code>min_q + (max_q - min_q) * ((step / (q_steps - 1)) ** 1.5) for step
          in range(q_steps)</code>
      </p>
      <p>
        Rounding our results to integers (necessary with current SVT-AV1) gave
        us 5 CRF values between 20 and 30, according to my input. We use this
        formula to focus more of our data points on higher fidelity encodes,
        where the difference in filesize may be larger for smaller differences
        in fidelity. This is more helpful when working with much higher fidelity
        than we care about here, but it is a good thing to remember, because we
        want a curve with less data points to look more like one with a greater
        number of data points.
      </p>
      <p>
        Now, we can compare encoders by generating multiple curves. We have the
        data, and we can use <code>plot.py</code> with our data inputs for a
        simple plot.
      </p>
      <p>
        For a hobbyist use case, this may be a fine place to stop. If it encodes
        in reasonable time, and it is closer to the upper left on the curve, it
        may satisfy you to use the more compression efficient encoder. But, what
        about at production scale, where you care more about time?
      </p>
      <h2>Comparing Overall Efficiency</h2>
      <p>Before moving on, consider what we need for this graph:</p>
      <ul>
        <li>A value representing the metric difference between two curves</li>
        <li>
          A value representing the time difference between the
          encoder/configuration used for each curve
        </li>
        <li>
          A graph that compares these two for various steps on a curve, which we
          can use to compare other curves for other encoders
        </li>
      </ul>
      <p>
        That final encoder curve describes an encoder's overall efficiency
        according to a given metric. Now, let's explore how to gather each
        value.
      </p>
      <p>
        <strong>BD-Rate</strong> (Bjontegaard Delta Rate) is a way to compare
        the efficiency of two curves. It answers the question: "For the same
        quality level, how much more or less data does method B need compared to
        method A?"
      </p>
      <p>
        If you stopped at the end of the previous section and ran <code
        >plot.py</code>, you'd notice that it provides BD-Rate numbers for each
        stats file you provided it, relative to the first stats file. So, if the
        BD-Rate is -20% between A & B, it means the second method needs 20% less
        data to achieve the same quality as the first method, which is a good
        improvement.
      </p>
      <p>
        <code>plot.py</code> writes these BD-Rate values to a CSV, along with
        the average time computed across the encodes in each stats file.
      </p>
      <p>
        Now, your next <code>plot.py</code> invocation (for the next stats files
        belonging to the next encoder) <em>needs</em> to use the <em
        >previous</em> worst stats file as the first argument in order to
        compute BD-Rates relative to the encoder you're now comparing against.
        You'll get another CSV output.
      </p>
      <p>
        Here's an example result, comparing SVT-AV1 v3.0.0 to SVT-AV1-PSY
        v2.3.0-B:
      </p>
      <img
        src="/static/images/encoder_efficiency.svg"
        alt="SVT-AV1 BD-Rate Plot"
      >
      <p>
        You can see that SVT-AV1 v3.0.0 is able to produce smaller BD-Rates
        relative to v2.3.0-B in less time, so it would be considered the more
        efficient encoder, according to W-XPSNR. Again, even though Preset 10 in
        SVT-AV1 v3.0.0 has a worse BD-Rate than Preset 10 in SVT-AV1-PSY
        v2.3.0-B, the time difference means that along the curve SVT-AV1 v3.0.0
        is more efficient overall.
      </p>
      <h2>Conclusion</h2>
      <p>
        That's all for now. I hope you found this blog post helpful in
        understanding how to compare encoders, because it is a crucial part of
        encoder development and can help you make an informed decision about
        which encoder to use for your specific needs. Happy encoding!
      </p>

      <div class="sponsor-widget">
        <a href="https://github.com/gianni-rosato" class="profile-section">
          <img
            src="/static/images/my_gh_profile.avif"
            alt="Gianni Rosato"
            class="profile-photo"
          />
        </a>
        <div class="content-section">
          <h3 class="profile-name">Sponsor Me on GitHub Sponsors</h3>
          Help support my open source efforts - a little goes a long way!
        </div>
        <a
          href="https://github.com/sponsors/gianni-rosato?o=esc"
          class="sponsor-button"
        >
          <svg
            class="heart-icon"
            xmlns="http://www.w3.org/2000/svg"
            width="16"
            height="16"
            fill="none"
            viewBox="0 0 0.48 0.48"
          >
            <path
              stroke="#000"
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width=".043"
              d="M.24.11C.202.063.138.05.09.09a.118.118 0 0 0-.017.161 1.798 1.798 0 0 0 .164.162.01.01 0 0 0 .006 0L.25.408C.28.38.376.294.407.252A.117.117 0 0 0 .39.092C.341.05.278.063.24.108Z"
              clip-rule="evenodd"
              style="stroke: oklch(70.2%0.35 328.4); stroke-opacity: 1"
            />
          </svg>
          Sponsor
        </a>
      </div>
      <p>
        <a href="https://disobey.net/@gianni" target="_blank">Mastodon</a> | <a
          href="https://matrix.to/#/@computerbustr:matrix.org"
          target="_blank"
        >Matrix</a> | <a href="https://wiki.x266.mov/" target="_blank"
        >The Codec Wiki</a> | <a href="https://svt-av1-psy.com/" target="_blank"
        >SVT-AV1-PSY</a>
      </p>
    </div>
  </body>
</html>
